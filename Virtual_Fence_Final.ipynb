{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMBSRN5kyGjaiPXOvLzjP7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smze/Virtual_Fence1_Final/blob/main/Virtual_Fence_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ù†ØµØ¨ Ùˆ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙˆÛŒØ¯ÛŒÙˆØŒ YOLOv8ØŒ VLMØŒ MobileNet Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒØªØ§Ø³Øª"
      ],
      "metadata": {
        "id": "nVekkclt_rFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Part 1: Ù†ØµØ¨ Ùˆ ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\n",
        "########################################\n",
        "# Ù†ØµØ¨ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\n",
        "!pip install -q ultralytics opencv-python-headless tqdm pillow pyyaml torch torchvision torchaudio matplotlib seaborn albumentations scikit-learn\n",
        "\n",
        "# ÙˆØ§Ø±Ø¯ Ú©Ø±Ø¯Ù† Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§\n",
        "import os, json, random, shutil, zipfile, yaml\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import torch\n",
        "from google.colab import drive, files\n"
      ],
      "metadata": {
        "id": "QohwMgLv_iKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 2: Ù…ÙˆÙ†Øª Ú©Ø±Ø¯Ù† Google Drive Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ"
      ],
      "metadata": {
        "id": "RGJV25hD_rtO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ Ø§Ø² Drive Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ø§Ø¯ØºØ§Ù… Ø¨Ø§ CrowdHuman Ø§Ø³Øª\n"
      ],
      "metadata": {
        "id": "cJ3iQUx-_5TP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ù…ÙˆÙ†Øª Ú©Ø±Ø¯Ù† Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ZIP Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ\n",
        "ZIP_PATH = \"/content/drive/MyDrive/Virtual_Fence/my_custom_yolo.zip\"\n",
        "BASE_CUSTOM = \"/content/my_custom_yolo\"\n",
        "os.makedirs(BASE_CUSTOM, exist_ok=True)\n",
        "\n",
        "# Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ\n",
        "with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:\n",
        "    zip_ref.extractall(BASE_CUSTOM)\n",
        "print(\"âœ… Custom dataset extracted from Drive!\")\n"
      ],
      "metadata": {
        "id": "9uN2SPl4_n-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø¯ÛŒØªØ§Ø³Øª CrowdHuman COCO Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ù…Ø³ÛŒØ±Ù‡Ø§ÛŒ train/val Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯\n",
        "\n",
        "Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§ÛŒ YOLO Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ ØªØµØ§ÙˆÛŒØ± Ùˆ Ø¨Ø±Ú†Ø³Ø¨â€ŒÙ‡Ø§ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯"
      ],
      "metadata": {
        "id": "KR-849d7AGmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 3: Ø¯Ø§Ù†Ù„ÙˆØ¯ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ CrowdHuman COCO\n",
        "# Ø¢Ù¾Ù„ÙˆØ¯ kaggle.json Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ API Kaggle\n",
        "files.upload()  # ÙØ§ÛŒÙ„ kaggle.json Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "# Ø¯Ø§Ù†Ù„ÙˆØ¯ CrowdHuman COCO\n",
        "!kaggle datasets download -d kakkimch/crowdhuman-coco -p /content\n",
        "!mkdir -p /content/crowdhuman\n",
        "!unzip -q /content/crowdhuman-coco.zip -d /content/crowdhuman\n",
        "\n",
        "# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§\n",
        "CROWD_PATH = \"/content/crowdhuman/CrowdHuman\"\n",
        "TRAIN_ODGT = os.path.join(CROWD_PATH, \"annotation_train.odgt\")\n",
        "VAL_ODGT = os.path.join(CROWD_PATH, \"annotation_val.odgt\")\n",
        "IMAGES_DIR_TRAIN = os.path.join(CROWD_PATH, \"Images/Train\")\n",
        "IMAGES_DIR_VAL = os.path.join(CROWD_PATH, \"Images/Val\")\n",
        "\n",
        "# Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ YOLO\n",
        "OUTPUT_BASE = \"/content/CrowdHuman_YOLO\"\n",
        "LABELS_TRAIN = os.path.join(OUTPUT_BASE, \"labels/train\")\n",
        "LABELS_VAL = os.path.join(OUTPUT_BASE, \"labels/val\")\n",
        "IMAGES_TRAIN = os.path.join(OUTPUT_BASE, \"images/train\")\n",
        "IMAGES_VAL = os.path.join(OUTPUT_BASE, \"images/val\")\n",
        "\n",
        "# Ø³Ø§Ø®Øª Ù¾ÙˆØ´Ù‡â€ŒÙ‡Ø§\n",
        "for p in [LABELS_TRAIN, LABELS_VAL, IMAGES_TRAIN, IMAGES_VAL]:\n",
        "   os.makedirs(p, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "jaXe3AGK_18H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Part 4: ØªØ§Ø¨Ø¹ ØªØ¨Ø¯ÛŒÙ„ .odgt Ø¨Ù‡ YOLO Ùˆ Ú©Ù¾ÛŒ ØªØµØ§ÙˆÛŒØ±\n",
        "def convert_odgt_to_yolo(odgt_file, output_dir, images_dir, images_out_dir):\n",
        "    with open(odgt_file, \"r\") as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    print(f\"ğŸ“„ Found {len(lines)} annotation lines in {odgt_file}\")\n",
        "\n",
        "    for line in tqdm(lines, desc=f\"Processing {os.path.basename(odgt_file)}\"):\n",
        "        data = json.loads(line)\n",
        "        image_name = data[\"ID\"] + \".jpg\"\n",
        "        img_path = os.path.join(images_dir, image_name)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            continue\n",
        "\n",
        "        # Ø®ÙˆØ§Ù†Ø¯Ù† Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØµÙˆÛŒØ±\n",
        "        try:\n",
        "            with Image.open(img_path) as img:\n",
        "                w, h = img.size\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "        # Ú©Ù¾ÛŒ ØªØµÙˆÛŒØ± Ø¨Ù‡ Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ\n",
        "        shutil.copy(img_path, os.path.join(images_out_dir, image_name))\n",
        "\n",
        "        # Ø³Ø§Ø®Øª ÙØ§ÛŒÙ„ txt YOLO\n",
        "        label_path = os.path.join(output_dir, image_name.replace(\".jpg\", \".txt\"))\n",
        "        with open(label_path, \"w\") as out:\n",
        "            for gtbox in data[\"gtboxes\"]:\n",
        "                if \"head_attr\" in gtbox and gtbox[\"head_attr\"].get(\"ignore\", 0) == 1:\n",
        "                    continue\n",
        "                if \"fbox\" not in gtbox:\n",
        "                    continue\n",
        "                x, y, bw, bh = gtbox[\"fbox\"]\n",
        "                x_center = (x + bw / 2) / w\n",
        "                y_center = (y + bh / 2) / h\n",
        "                w_norm = bw / w\n",
        "                h_norm = bh / h\n",
        "                class_id = 0\n",
        "                out.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n"
      ],
      "metadata": {
        "id": "k3XXqDHUAI0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 5: Ø§Ø¬Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ø±Ø§ÛŒ CrowdHuman\n",
        "convert_odgt_to_yolo(TRAIN_ODGT, LABELS_TRAIN, IMAGES_DIR_TRAIN, IMAGES_TRAIN)\n",
        "convert_odgt_to_yolo(VAL_ODGT, LABELS_VAL, IMAGES_DIR_VAL, IMAGES_VAL)\n",
        "print(\"âœ… CrowdHuman converted to YOLO format successfully!\")\n"
      ],
      "metadata": {
        "id": "F9re6Eh1ARog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 6: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ Ùˆ Ø§Ø¯ØºØ§Ù…\n",
        "SRC_IMAGES = os.path.join(BASE_CUSTOM, \"images\")\n",
        "SRC_LABELS = os.path.join(BASE_CUSTOM, \"labels\")\n",
        "\n",
        "all_images = glob(SRC_IMAGES + \"/*.jpg\")\n",
        "random.shuffle(all_images)\n",
        "split_idx = int(0.8 * len(all_images))  # 80% train, 20% val\n",
        "\n",
        "for i, img_path in enumerate(all_images):\n",
        "    fname = os.path.basename(img_path)\n",
        "    lbl_path = os.path.join(SRC_LABELS, fname.replace(\".jpg\", \".txt\"))\n",
        "    if i < split_idx:\n",
        "        shutil.move(img_path, IMAGES_TRAIN + \"/\" + fname)\n",
        "        shutil.move(lbl_path, LABELS_TRAIN + \"/\" + fname.replace(\".jpg\", \".txt\"))\n",
        "    else:\n",
        "        shutil.move(img_path, IMAGES_VAL + \"/\" + fname)\n",
        "        shutil.move(lbl_path, LABELS_VAL + \"/\" + fname.replace(\".jpg\", \".txt\"))\n",
        "\n",
        "print(\"âœ… Custom dataset merged into CrowdHuman successfully!\")\n"
      ],
      "metadata": {
        "id": "xapRkqXwAYlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part 7: Ø³Ø§Ø®Øª train.txtØŒ val.txt Ùˆ data.yaml"
      ],
      "metadata": {
        "id": "7Lt5q24JAdHH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 7: Ø³Ø§Ø®Øª train.txtØŒ val.txt Ùˆ data.yaml\n",
        "TRAIN_TXT = os.path.join(OUTPUT_BASE, \"train.txt\")\n",
        "VAL_TXT = os.path.join(OUTPUT_BASE, \"val.txt\")\n",
        "\n",
        "with open(TRAIN_TXT, \"w\") as f:\n",
        "    for img in glob(IMAGES_TRAIN + \"/*.jpg\"):\n",
        "        f.write(img + \"\\n\")\n",
        "\n",
        "with open(VAL_TXT, \"w\") as f:\n",
        "    for img in glob(IMAGES_VAL + \"/*.jpg\"):\n",
        "        f.write(img + \"\\n\")\n",
        "\n",
        "DATA_YAML = os.path.join(OUTPUT_BASE, \"crowdhuman_merged.yaml\")\n",
        "data_dict = {\n",
        "    'train': TRAIN_TXT,\n",
        "    'val': VAL_TXT,\n",
        "    'nc': 1,\n",
        "    'names': ['person']\n",
        "}\n",
        "with open(DATA_YAML, 'w') as f:\n",
        "    yaml.dump(data_dict, f)\n",
        "\n",
        "print(\"âœ… Dataset prepared and YAML created at:\", DATA_YAML)\n"
      ],
      "metadata": {
        "id": "o8nWTcxbAd1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 8: Ø¢Ù…ÙˆØ²Ø´ YOLOv8 Ø¨Ø§ ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ pre-trained\n",
        "##ÙˆØ²Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø² Ù¾ÛŒØ´ Ø¢Ù…ÙˆØ²Ø´ Ø¯ÛŒØ¯Ù‡ (yolov8n.pt) Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯\n",
        "###Ø¢Ù…ÙˆØ²Ø´ Ø±ÙˆÛŒ Ø¯ÛŒØªØ§Ø³Øª ØªØ±Ú©ÛŒØ¨ÛŒ CrowdHuman + Ø¯ÛŒØªØ§Ø³Øª Ø³ÙØ§Ø±Ø´ÛŒ###\n",
        "\n",
        "# Ù†ØµØ¨ YOLOv8\n",
        "!pip install -U ultralytics\n",
        "\n",
        "# Ø¢Ù…ÙˆØ²Ø´ YOLOv8n Ø¨Ø§ Ø¯ÛŒØªØ§Ø³Øª ØªØ±Ú©ÛŒØ¨ÛŒ\n",
        "!yolo train \\\n",
        "data=/content/CrowdHuman_YOLO/crowdhuman_merged.yaml \\\n",
        "model=yolov8n.pt \\\n",
        "epochs=50 \\\n",
        "imgsz=640 \\\n",
        "batch=16 \\\n",
        "name=crowdhuman_merged\n"
      ],
      "metadata": {
        "id": "e5WvpAU0Akuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 9: Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ù†Ø·Ù‚Ù‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (Virtual Fence\n",
        "# ØªØ¹Ø±ÛŒÙ Ù…Ù†Ø·Ù‚Ù‡ Ù…Ø³ØªØ·ÛŒÙ„ÛŒ Ø¯Ø± ÙˆÛŒØ¯ÛŒÙˆ\n",
        "fence_coords = (100, 200, 500, 600)  # (x1, y1, x2, y2) Ù‚Ø§Ø¨Ù„ ØªØºÛŒÛŒØ±\n",
        "\n",
        "def draw_fence(frame, coords=fence_coords, color=(0,255,0), thickness=2):\n",
        "    cv2.rectangle(frame, (coords[0], coords[1]), (coords[2], coords[3]), color, thickness)\n"
      ],
      "metadata": {
        "id": "eM0lSwhJA2Cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Part 10: ØªØ¹Ø±ÛŒÙ Ú©Ù„Ø§Ø³ Ø´Ù…Ø§Ø±Ù†Ø¯Ù‡ Ùˆ Ø±Ø¯ÛŒØ§Ø¨ Ø¨Ø±Ø§ÛŒ Ø§ÙØ±Ø§Ø¯\n",
        "class PersonCounter:\n",
        "    def __init__(self, fence_coords):\n",
        "        self.fence_coords = fence_coords\n",
        "        self.counted_ids = set()\n",
        "        self.total_count = 0\n",
        "\n",
        "    def update(self, detections):\n",
        "        # detections: list of dicts with 'id', 'bbox' keys\n",
        "        for det in detections:\n",
        "            cx = (det['bbox'][0] + det['bbox'][2]) // 2\n",
        "            cy = (det['bbox'][1] + det['bbox'][3]) // 2\n",
        "            if (self.fence_coords[0] <= cx <= self.fence_coords[2] and\n",
        "                self.fence_coords[1] <= cy <= self.fence_coords[3] and\n",
        "                det['id'] not in self.counted_ids):\n",
        "                self.counted_ids.add(det['id'])\n",
        "                self.total_count += 1\n",
        "        return self.total_count\n",
        "\n"
      ],
      "metadata": {
        "id": "xkCkBoLXA6gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Part 11: Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ù‡ Ø±ÙˆÛŒÚ©Ø±Ø¯"
      ],
      "metadata": {
        "id": "OVp2VakkA-3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Cell A â€” ØªÙˆØ§Ø¨Ø¹ Ù…Ø´ØªØ±Ú© Ùˆ Ú©Ù„Ø§Ø³ Tracker/Counter\n",
        "(Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø§ÙˆÙ„ Ø§Ø¬Ø±Ø§ Ø´ÙˆØ¯ â€” ØªÙˆØ§Ø¨Ø¹ Ùˆ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ Ø¨ÛŒÙ† Ù‡Ù…Ù‡ Ø±ÙˆØ´â€ŒÙ‡Ø§ Ù…Ø´ØªØ±Ú©â€ŒØ§Ù†Ø¯)\n",
        "# ===========================\n",
        "# Cell A: Shared utilities\n",
        "# ===========================\n",
        "import time, math\n",
        "import numpy as np\n",
        "import cv2\n",
        "from collections import OrderedDict\n",
        "\n",
        "def iou(boxA, boxB):\n",
        "    # box: [x1,y1,x2,y2]\n",
        "    xA = max(boxA[0], boxB[0]); yA = max(boxA[1], boxB[1])\n",
        "    xB = min(boxA[2], boxB[2]); yB = min(boxA[3], boxB[3])\n",
        "    interW = max(0, xB - xA); interH = max(0, yB - yA)\n",
        "    interArea = interW * interH\n",
        "    boxAArea = max(0, (boxA[2]-boxA[0])) * max(0, (boxA[3]-boxA[1]))\n",
        "    boxBArea = max(0, (boxB[2]-boxB[0])) * max(0, (boxB[3]-boxB[1]))\n",
        "    denom = float(boxAArea + boxBArea - interArea + 1e-6)\n",
        "    return interArea / denom if denom>0 else 0.0\n",
        "\n",
        "class SimpleTracker:\n",
        "    \"\"\"\n",
        "    Simple IoU-based tracker:\n",
        "    - Keeps dictionary of tracked objects: id -> {'bbox':bbox, 'age':0, 'missed':0, 'counted':False}\n",
        "    - For each frame: match detections to existing objects by IoU threshold\n",
        "    - Unmatched detections -> new IDs\n",
        "    - Unmatched tracks -> increase missed count, remove if missed > max_missed\n",
        "    \"\"\"\n",
        "    def __init__(self, iou_threshold=0.3, max_missed=5):\n",
        "        self.next_id = 0\n",
        "        self.tracks = OrderedDict()\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.max_missed = max_missed\n",
        "\n",
        "    def update(self, detections):\n",
        "        # detections: list of bboxes [[x1,y1,x2,y2], ...] (integers)\n",
        "        updated = OrderedDict()\n",
        "        used_det = set()\n",
        "\n",
        "        # Match existing tracks -> detections by best IoU\n",
        "        for tid, t in list(self.tracks.items()):\n",
        "            best_iou = 0.0; best_j = -1\n",
        "            for j, det in enumerate(detections):\n",
        "                if j in used_det: continue\n",
        "                val = iou(t['bbox'], det)\n",
        "                if val > best_iou:\n",
        "                    best_iou = val; best_j = j\n",
        "            if best_iou >= self.iou_threshold and best_j >= 0:\n",
        "                # matched\n",
        "                updated[tid] = {'bbox': detections[best_j], 'age': t['age']+1, 'missed': 0, 'counted': t.get('counted', False)}\n",
        "                used_det.add(best_j)\n",
        "            else:\n",
        "                # no match -> increase missed\n",
        "                if t['missed']+1 <= self.max_missed:\n",
        "                    updated[tid] = {'bbox': t['bbox'], 'age': t['age']+1, 'missed': t['missed']+1, 'counted': t.get('counted', False)}\n",
        "                # else: drop track\n",
        "\n",
        "        # Add new detections as new tracks\n",
        "        for j, det in enumerate(detections):\n",
        "            if j in used_det: continue\n",
        "            updated[self.next_id] = {'bbox': det, 'age': 1, 'missed': 0, 'counted': False}\n",
        "            self.next_id += 1\n",
        "\n",
        "        # Replace tracks\n",
        "        self.tracks = updated\n",
        "        return self.tracks\n",
        "\n",
        "    def reset(self):\n",
        "        self.next_id = 0\n",
        "        self.tracks = OrderedDict()\n"
      ],
      "metadata": {
        "id": "MiZh5ZDaBPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§ÛŒÙ† tracker Ø³Ø§Ø¯Ù‡ Ùˆ Ú©Ø§Ø±Ø§ Ø§Ø³Øª â€” Ø³Ø±ÛŒØ¹ØŒ Ø¨Ø¯ÙˆÙ† ÙˆØ§Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ø§Ø±Ø¬ÛŒØŒ Ùˆ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±Ø¯Ù‡Ø§ÛŒ real-time Ù…Ù†Ø§Ø³Ø¨ Ø§Ø³Øª. Ø§Ø² IoU Ø¨Ø±Ø§ÛŒ Ø§ØªØµØ§Ù„ Ø§Ø´ÛŒØ§Ø¡ Ø¯Ø± ÙØ±ÛŒÙ…â€ŒÙ‡Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
      ],
      "metadata": {
        "id": "oclawUR3BWjg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Cell B â€” Ø§Ø¬Ø±Ø§ÛŒ YOLOv8n (pre-trained ÛŒØ§ fine-tuned) Ø¨Ø§ tracker Ùˆ Ø´Ù…Ø§Ø±Ø´\n",
        "# ===========================\n",
        "# Cell B: YOLOv8n inference + SimpleTracker + counting\n",
        "# ===========================\n",
        "from ultralytics import YOLO\n",
        "import os, time\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ…â€ŒÙ‡Ø§ (ÙˆÛŒØ±Ø§ÛŒØ´ Ú©Ù† Ø§Ú¯Ø± Ù„Ø§Ø²Ù… Ø§Ø³Øª)\n",
        "INPUT_VIDEO = \"/content/input.mp4\"                # Ù…Ø³ÛŒØ± ÙˆÛŒØ¯ÛŒÙˆÛŒ ÙˆØ±ÙˆØ¯ÛŒ\n",
        "OUTPUT_VIDEO_YOLO = \"/content/output_yolo.mp4\"   # Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ\n",
        "FENCE = (200, 100, 600, 400)                     # (x1,y1,x2,y2) â€” ØªÙ†Ø¸ÛŒÙ… Ø¯Ù„Ø®ÙˆØ§Ù‡\n",
        "YOLO_WEIGHT = \"yolov8n.pt\"                       # ÛŒØ§ Ù…Ø³ÛŒØ± Ø¨Ù‡ weight Ø¢Ù…ÙˆØ²Ø´ÛŒ Ø´Ù…Ø§\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ YOLO\n",
        "yolo_model = YOLO(YOLO_WEIGHT)\n",
        "\n",
        "# Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ ÙˆÛŒØ¯ÛŒÙˆ\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_YOLO, fourcc, fps, (width, height))\n",
        "\n",
        "# tracker Ùˆ counter\n",
        "tracker = SimpleTracker(iou_threshold=0.3, max_missed=5)\n",
        "counted_ids = set()\n",
        "total_count = 0\n",
        "\n",
        "start_time = time.time()\n",
        "frame_idx = 0\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_idx += 1\n",
        "\n",
        "    # run YOLO inference (use small batch by single frame)\n",
        "    results = yolo_model.predict(frame, conf=0.4, classes=[0], verbose=False)  # only person class\n",
        "    dets = []\n",
        "    for res in results:\n",
        "        # each res.boxes contain many boxes\n",
        "        boxes = res.boxes.xyxy.cpu().numpy()\n",
        "        confs = res.boxes.conf.cpu().numpy()\n",
        "        for b, c in zip(boxes, confs):\n",
        "            if c < 0.25: continue\n",
        "            x1,y1,x2,y2 = b.astype(int)\n",
        "            dets.append([x1,y1,x2,y2])\n",
        "\n",
        "    # update tracker\n",
        "    tracks = tracker.update(dets)\n",
        "\n",
        "    # draw & count\n",
        "    x1f,y1f,x2f,y2f = FENCE\n",
        "    cv2.rectangle(frame, (x1f,y1f), (x2f,y2f), (0,255,0), 2)\n",
        "    for tid, t in tracks.items():\n",
        "        bx = t['bbox']\n",
        "        bx1,by1,bx2,by2 = map(int, bx)\n",
        "        cx = int((bx1+bx2)/2); cy = int((by1+by2)/2)\n",
        "        # draw\n",
        "        cv2.rectangle(frame, (bx1,by1),(bx2,by2),(0,0,255),2)\n",
        "        cv2.circle(frame, (cx,cy), 3, (255,0,0), -1)\n",
        "        cv2.putText(frame, f\"ID:{tid}\", (bx1, max(0,by1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 1)\n",
        "\n",
        "        # count if entering fence and not counted\n",
        "        if (x1f <= cx <= x2f and y1f <= cy <= y2f) and (not t['counted']):\n",
        "            total_count += 1\n",
        "            t['counted'] = True\n",
        "            tracker.tracks[tid]['counted'] = True  # persist\n",
        "\n",
        "    # overlay counter\n",
        "    cv2.putText(frame, f\"YOLO Count: {total_count}\", (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0,0,255), 3)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "elapsed_yolo = time.time() - start_time\n",
        "print(f\"âœ… YOLO done â€” Count: {total_count}, Time: {elapsed_yolo:.2f}s, Output: {OUTPUT_VIDEO_YOLO}\")\n",
        "# store results for benchmark\n",
        "yolo_results = {'count': total_count, 'time': elapsed_yolo, 'output': OUTPUT_VIDEO_YOLO}\n"
      ],
      "metadata": {
        "id": "qjIxaIsVBaxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§ÛŒÙ† Ø¬Ø±ÛŒØ§Ù† Ø§Ø² ÙˆØ²Ù† YOLO_WEIGHT Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ù…Ø³ÛŒØ± ÙˆØ²Ù† fine-tuned\n",
        "  Ø®ÙˆØ¯ Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† yolov8n.pt Ú©Ù†ÛŒÙ…).\n",
        "\n",
        "tracker Ø¯Ø±ÙˆÙ† Ø­Ù„Ù‚Ù‡ Ø¨Ø±ÙˆØ² Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ ØªÙ…Ø§Ù… Ù…Ù†Ø·Ù‚ Ø´Ù…Ø§Ø±Ø´ (Ù…Ø±Ú©Ø² Ø¬Ø¹Ø¨Ù‡ Ø¯Ø§Ø®Ù„ fence) Ø¯Ø± Ù‡Ù…ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n",
        "\n",
        "Ø®Ø±ÙˆØ¬ÛŒ ÙˆÛŒØ¯ÛŒÙˆ Ùˆ Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯."
      ],
      "metadata": {
        "id": "b1D8CAvSBluo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Cell C â€” Ø§Ø¬Ø±Ø§ÛŒ MobileNet-SSD (ssdlite320_mobilenet_v3_large) + tracker + counting\n",
        "\n",
        "##(Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø³Ø¨Ú© Ùˆ Ø³Ø±ÛŒØ¹ Ø§Ø³ØªØ› Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÚ¯Ø§Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø¯ÙˆØ¯ â€” Ø§Ø² torchvision pretrained Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯)\n",
        "# ===========================\n",
        "# Cell C: MobileNet-SSD (ssdlite320_mobilenet_v3_large) inference + tracking + counting\n",
        "# ===========================\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.models.detection import ssdlite320_mobilenet_v3_large\n",
        "\n",
        "# ØªÙ†Ø¸ÛŒÙ…Ø§Øª\n",
        "INPUT_VIDEO = \"/content/input.mp4\"\n",
        "OUTPUT_VIDEO_MOB = \"/content/output_mobilenet.mp4\"\n",
        "FENCE = (200, 100, 600, 400)   # Ù‡Ù…Ø§Ù† Ù…Ù†Ø·Ù‚Ù‡ ÛŒØ§ Ø¯Ù„Ø®ÙˆØ§Ù‡\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ø² torchvision (pretrained COCO)\n",
        "mob_model = ssdlite320_mobilenet_v3_large(pretrained=True).to(device)\n",
        "mob_model.eval()\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps    = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(OUTPUT_VIDEO_MOB, fourcc, fps, (width, height))\n",
        "\n",
        "tracker = SimpleTracker(iou_threshold=0.3, max_missed=5)\n",
        "total_count_m = 0\n",
        "start_time = time.time()\n",
        "frame_idx = 0\n",
        "\n",
        "while True:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret: break\n",
        "    frame_idx += 1\n",
        "\n",
        "    # prepare input\n",
        "    img_t = transform(frame).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = mob_model([img_t])[0]  # dict with boxes, scores, labels\n",
        "\n",
        "    dets = []\n",
        "    boxes = outputs['boxes'].cpu().numpy()\n",
        "    scores = outputs['scores'].cpu().numpy()\n",
        "    labels = outputs['labels'].cpu().numpy()\n",
        "    for b, s, lab in zip(boxes, scores, labels):\n",
        "        # COCO label for person is 1\n",
        "        if lab == 1 and s > 0.4:\n",
        "            x1,y1,x2,y2 = b.astype(int)\n",
        "            dets.append([x1,y1,x2,y2])\n",
        "\n",
        "    tracks = tracker.update(dets)\n",
        "\n",
        "    # draw\n",
        "    x1f,y1f,x2f,y2f = FENCE\n",
        "    cv2.rectangle(frame, (x1f,y1f), (x2f,y2f), (0,255,0), 2)\n",
        "    for tid, t in tracks.items():\n",
        "        bx1,by1,bx2,by2 = map(int, t['bbox'])\n",
        "        cx = (bx1+bx2)//2; cy = (by1+by2)//2\n",
        "        cv2.rectangle(frame, (bx1,by1),(bx2,by2),(255,0,0),2)\n",
        "        cv2.putText(frame, f\"ID:{tid}\", (bx1, max(0,by1-8)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255,255,0),1)\n",
        "        if (x1f <= cx <= x2f and y1f <= cy <= y2f) and (not t['counted']):\n",
        "            total_count_m += 1\n",
        "            t['counted'] = True\n",
        "            tracker.tracks[tid]['counted'] = True\n",
        "\n",
        "    cv2.putText(frame, f\"MobileNet Count: {total_count_m}\", (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 3)\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release(); out.release()\n",
        "elapsed_mob = time.time() - start_time\n",
        "print(f\"âœ… MobileNet-SSD done â€” Count: {total_count_m}, Time: {elapsed_mob:.2f}s, Output: {OUTPUT_VIDEO_MOB}\")\n",
        "mobilenet_results = {'count': total_count_m, 'time': elapsed_mob, 'output': OUTPUT_VIDEO_MOB}\n"
      ],
      "metadata": {
        "id": "f0Dr9HmpCMT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§Ø² Ù…Ø¯Ù„ ssdlite320_mobilenet_v3_large Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± torchvision Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….\n",
        "\n",
        "Ø³Ø¨Ú© Ùˆ Ø³Ø±ÛŒØ¹ Ø§Ø³ØªØ› Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ YOLOv8n.\n",
        "\n",
        "Ø¢Ø³ØªØ§Ù†Ù‡ Ù†Ù…Ø±Ù‡ (score) Ø±Ø§ 0.4 Ù‚Ø±Ø§Ø± Ø¯Ø§Ø¯Ù…Ø› Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¢Ù† Ø±Ø§ Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø¯ ØªØ§ false-positive Ú©Ù…ØªØ± Ø´ÙˆØ¯."
      ],
      "metadata": {
        "id": "LfAmflgGCYKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###Cell D â€” Ø§Ø¬Ø±Ø§ÛŒ OMNI VLM\n",
        "# ===========================\n",
        "# Cell D: VLM-based detection (optional) â€” safe fallback if not available\n",
        "# ===========================\n",
        "import importlib, sys\n",
        "vlm_available = False\n",
        "time_vlm = None\n",
        "vlm_count = 0\n",
        "OUTPUT_VIDEO_VLM = \"/content/output_vlm.mp4\"\n",
        "\n",
        "try:\n",
        "    # Try to import groundingdino from huggingface transformers pipeline or custom libs\n",
        "    from transformers import AutoProcessor, AutoModelForObjectDetection\n",
        "    # <-- NOTE: model name must be replaced with a valid HF model that supports object detection grounding.\n",
        "    # Example placeholder name (may need to be changed to an available checkpoint):\n",
        "    MODEL_NAME = \"ShilongLiu/groundingdino-swinb-coco\"  # placeholder; replace if you have exact model\n",
        "    print(\"Attempting to load VLM model:\", MODEL_NAME)\n",
        "    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n",
        "    vlm_model = AutoModelForObjectDetection.from_pretrained(MODEL_NAME)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    vlm_model.to(device)\n",
        "    vlm_model.eval()\n",
        "    vlm_available = True\n",
        "except Exception as e:\n",
        "    print(\"âš ï¸ VLM model not available in this environment or AutoModel config incompatible.\")\n",
        "    print(\"To enable VLM, install the appropriate HF model and update MODEL_NAME variable to a compatible grounding-detection model.\")\n",
        "    print(\"Exception:\", e)\n",
        "    vlm_available = False\n",
        "\n",
        "if vlm_available:\n",
        "    cap = cv2.VideoCapture(INPUT_VIDEO)\n",
        "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps    = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(OUTPUT_VIDEO_VLM, fourcc, fps, (width, height))\n",
        "\n",
        "    tracker = SimpleTracker(iou_threshold=0.3, max_missed=5)\n",
        "    vlm_count = 0\n",
        "    start_time = time.time()\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "        # Prepare processor inputs (text prompt)\n",
        "        inputs = processor(images=image_rgb, text=[\"person\"], return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = vlm_model(**inputs)\n",
        "        # Parse detections â€” parsing code depends on model output structure\n",
        "        # Try generic fields; if not present, adapt according to the HF model docs\n",
        "        try:\n",
        "            boxes = outputs.pred_boxes[0].cpu().numpy()  # may differ per model\n",
        "            scores = outputs.pred_logits[0].softmax(-1).max(-1).values.cpu().numpy()\n",
        "        except:\n",
        "            # fallback: try outputs[\"boxes\"], outputs[\"scores\"]\n",
        "            boxes = outputs.get(\"boxes\", np.array([]))\n",
        "            scores = outputs.get(\"scores\", np.array([]))\n",
        "\n",
        "        dets = []\n",
        "        for b, s in zip(boxes, scores):\n",
        "            if s < 0.3: continue\n",
        "            x1,y1,x2,y2 = b.astype(int)\n",
        "            dets.append([x1,y1,x2,y2])\n",
        "\n",
        "        tracks = tracker.update(dets)\n",
        "        x1f,y1f,x2f,y2f = FENCE\n",
        "        cv2.rectangle(frame, (x1f,y1f),(x2f,y2f), (0,255,0), 2)\n",
        "        for tid, t in tracks.items():\n",
        "            bx1,by1,bx2,by2 = map(int, t['bbox'])\n",
        "            cx = (bx1+bx2)//2; cy = (by1+by2)//2\n",
        "            cv2.rectangle(frame, (bx1,by1),(bx2,by2),(0,255,255),2)\n",
        "            if (x1f <= cx <= x2f and y1f <= cy <= y2f) and (not t['counted']):\n",
        "                vlm_count += 1\n",
        "                t['counted'] = True\n",
        "                tracker.tracks[tid]['counted'] = True\n",
        "\n",
        "        cv2.putText(frame, f\"VLM Count: {vlm_count}\", (10,40), cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0,0,255), 3)\n",
        "        out.write(frame)\n",
        "\n",
        "    cap.release(); out.release()\n",
        "    time_vlm = time.time() - start_time\n",
        "    print(f\"âœ… VLM done â€” Count: {vlm_count}, Time: {time_vlm:.2f}s, Output: {OUTPUT_VIDEO_VLM}\")\n",
        "    vlm_results = {'count': vlm_count, 'time': time_vlm, 'output': OUTPUT_VIDEO_VLM}\n",
        "else:\n",
        "    print(\"VLM step skipped. Provide a compatible HF grounding detection model and set MODEL_NAME to run this step.\")\n",
        "    vlm_results = {'count': None, 'time': None, 'output': None}\n"
      ],
      "metadata": {
        "id": "3ykC_NnNCd1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "VLMÙ‡Ø§ Ø±ÙˆÛŒ HuggingFace ØªÙ†ÙˆØ¹ Ø²ÛŒØ§Ø¯ÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ø± Ù…Ø¯Ù„ Ø³Ø§Ø®ØªØ§Ø± Ù…Ø®ØµÙˆØµÛŒ Ø¯Ø§Ø±Ø¯. Ù…Ù† ÛŒÚ© Ø¨Ù„ÙˆÚ© Ø§Ù…Ù† Ú¯Ø°Ø§Ø´ØªÙ… Ú©Ù‡ ØªÙ„Ø§Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ù…Ø¯Ù„ Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†Ø¯ Ùˆ Ø§Ú¯Ø± incompatible Ø¨Ø§Ø´Ø¯ØŒ Ù…Ø±Ø­Ù„Ù‡ Ø±Ø§ Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯ Ú©Ù‡ Ú†Ù‡ Ú©Ø§Ø±ÛŒ Ø¨Ø§ÛŒØ¯ Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒ (ØªØºÛŒÛŒØ± MODEL_NAME Ø¨Ù‡ ÛŒÚ© Ù…Ø¯Ù„ Ù…Ù†Ø§Ø³Ø¨ ÛŒØ§ Ù†ØµØ¨ Ù¾Ú©ÛŒØ¬â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù…).\n",
        "\n",
        "Ø§Ú¯Ø± Ù…Ø¯Ù„ VLM Ø¯Ø± Ù…Ø­ÛŒØ· Ù†ØµØ¨ Ø¨Ø§Ø´Ø¯ØŒ Ø§ÛŒÙ† Ø¨Ù„ÙˆÚ© Ø¢Ù† Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯Ø› Ø¯Ø± ØºÛŒØ± Ø§ÛŒÙ† ØµÙˆØ±Øª skip Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ùˆ Ù¾Ø±ÙˆÚ˜Ù‡ Ø§Ø¯Ø§Ù…Ù‡ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."
      ],
      "metadata": {
        "id": "WADKHNYfCoPo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HcAa_xePCrbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§ÛŒÙ† Ø³Ù„ÙˆÙ„ Ø³Ø¹ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ÛŒÚ© VLM Ø§Ø² HuggingFace Ù…Ø«Ù„ GroundingDINO ÛŒØ§ Ù…Ø¯Ù„ detection text-conditioned Ø±Ø§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ú©Ù†Ø¯Ø› Ø§Ú¯Ø± Ù…Ø¯Ù„/Ù¾Ú©ÛŒØ¬ Ø¯Ø± Ù…Ø­ÛŒØ· Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø¹Ù…Ù„ÛŒØ§Øª Ø±Ø§ Ù‡ÙˆØ´Ù…Ù†Ø¯Ø§Ù†Ù‡ Ø±Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ø¨Ù‡ ØªÙˆ Ù¾ÛŒØ§Ù… Ø±Ø§Ù‡Ù†Ù…Ø§ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."
      ],
      "metadata": {
        "id": "kfCUmKd-Ch7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##Cell E â€” Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ù†ØªØ§ÛŒØ¬ Ùˆ Ø¨Ù†Ú†Ù…Ø§Ø±Ú© (Ø¬Ø¯ÙˆÙ„ + Ù†Ù…ÙˆØ¯Ø§Ø±)"
      ],
      "metadata": {
        "id": "evdu97BwCu0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# Cell E: Benchmark table + plots\n",
        "# ===========================\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "rows = []\n",
        "# YOLO results\n",
        "rows.append({\n",
        "    'Method': 'YOLOv8n',\n",
        "    'Counted': yolo_results['count'],\n",
        "    'Time_s': yolo_results['time'],\n",
        "    'Output': yolo_results['output']\n",
        "})\n",
        "# MobileNet\n",
        "rows.append({\n",
        "    'Method': 'MobileNet-SSD',\n",
        "    'Counted': mobilenet_results['count'],\n",
        "    'Time_s': mobilenet_results['time'],\n",
        "    'Output': mobilenet_results['output']\n",
        "})\n",
        "# VLM (if any)\n",
        "rows.append({\n",
        "    'Method': 'OMNI_VLM',\n",
        "    'Counted': vlm_results.get('count'),\n",
        "    'Time_s': vlm_results.get('time'),\n",
        "    'Output': vlm_results.get('output')\n",
        "})\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "display(df)\n",
        "\n",
        "# Plot counts\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(data=df.dropna(subset=['Counted']), x='Method', y='Counted', palette='pastel')\n",
        "plt.title(\"Detected / Counted people per method\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()\n",
        "\n",
        "# Plot time\n",
        "plt.figure(figsize=(8,4))\n",
        "sns.barplot(data=df.dropna(subset=['Time_s']), x='Method', y='Time_s', palette='muted')\n",
        "plt.title(\"Processing time (seconds) per method\")\n",
        "plt.ylabel(\"Seconds\")\n",
        "plt.show()\n",
        "\n",
        "# Save benchmark CSV\n",
        "bench_path = \"/content/benchmark_results.csv\"\n",
        "df.to_csv(bench_path, index=False)\n",
        "print(\"âœ… Benchmark saved to\", bench_path)\n"
      ],
      "metadata": {
        "id": "nkWflCIbCw1_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###Cell F â€” Ù†Ú©Ø§Øª Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ README / Ø®Ø±ÙˆØ¬ÛŒâ€ŒÙ‡Ø§\n",
        "# ===========================\n",
        "# Cell F: Final notes, save outputs and README snippet\n",
        "# ===========================\n",
        "# ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø´Ø¯Ù‡:\n",
        "print(\"YOLO output:\", yolo_results.get('output'))\n",
        "print(\"MobileNet output:\", mobilenet_results.get('output'))\n",
        "print(\"VLM output:\", vlm_results.get('output'))\n",
        "\n",
        "# Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± ÛŒÚ© JSON Ø¨Ø±Ø§ÛŒ Ù…Ø³ØªÙ†Ø¯Ø³Ø§Ø²ÛŒ\n",
        "import json\n",
        "summary = {\n",
        "    'yolo': yolo_results,\n",
        "    'mobilenet': mobilenet_results,\n",
        "    'vlm': vlm_results,\n",
        "    'fence_coords': FENCE,\n",
        "    'input_video': INPUT_VIDEO\n",
        "}\n",
        "with open('/content/experiment_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"âœ… Summary saved: /content/experiment_summary.json\")\n",
        "\n",
        "# README snippet (Ú©ÙˆØªØ§Ù‡) Ø¨Ø±Ø§ÛŒ Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨\n",
        "readme_snippet = f\"\"\"\n",
        "# Virtual Fence - Experiment Summary\n",
        "\n",
        "Input video: {INPUT_VIDEO}\n",
        "Fence coords: {FENCE}\n",
        "\n",
        "Methods & outputs:\n",
        "- YOLOv8n: output saved at {yolo_results.get('output')}, time={yolo_results.get('time'):.2f}s, count={yolo_results.get('count')}\n",
        "- MobileNet-SSD: output saved at {mobilenet_results.get('output')}, time={mobilenet_results.get('time'):.2f}s, count={mobilenet_results.get('count')}\n",
        "- OMNI VLM: output saved at {vlm_results.get('output')}, time={vlm_results.get('time')}, count={vlm_results.get('count')}\n",
        "\n",
        "Benchmark CSV: /content/benchmark_results.csv\n",
        "\"\"\"\n",
        "with open('/content/README_EXPERIMENT.txt', 'w') as f:\n",
        "    f.write(readme_snippet)\n",
        "print(\"âœ… README_EXPERIMENT snippet created at /content/README_EXPERIMENT.txt\")\n"
      ],
      "metadata": {
        "id": "wm2pC6OyC3EZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WbWKiQZsAf03"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3etOOIkwARXf"
      }
    }
  ]
}