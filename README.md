#  Virtual Fence Project

##  Overview

The **Virtual Fence** project aims to detect, track, and count people entering a predefined region within a crowded street scene.
It processes a video input, detects people in real time, and counts how many individuals cross into a user-defined rectangular zone.

The output is a processed **MP4 video** showing:

* Bounding boxes around detected people,
* A highlighted counting zone, and
* A real-time counter displayed on-screen.

Three distinct methods are implemented and benchmarked:

1. **YOLOv5** â€“ A state-of-the-art object detector.
2. **OMNI VLM (Vision-Language Model)** â€“ A multimodal model for contextual person detection.
3. **MobileNet (Custom Lightweight Model)** â€“ A fast, optimized solution for **Raspberry Pi** and other edge devices.

---

## âš™ï¸ Features

âœ… Person detection and tracking across video frames
âœ… Zone monitoring and real-time counting
âœ… Video output with live counter overlay
âœ… Benchmarking and visual comparison between three methods
âœ… Optimized MobileNet model for Raspberry Pi

---

##  Frameworks & Tools

* **PyTorch** â€“ for model development and inference
* **YOLOv5** â€“ pretrained model fine-tuned on the combined dataset
* **OMNI VLM** â€“ for language-guided person detection
* **MobileNetV3** â€“ lightweight CNN customized for real-time inference on low-power devices
* **OpenCV** â€“ for video processing and visualization
* **NumPy / Matplotlib** â€“ for analytics and visualization

---

## ğŸ“‚ Dataset

Two data sources are used for training and evaluation:

1. **CrowdHuman** (Public Dataset)

   * A large-scale dataset of crowded human scenes
   * Source: [https://huggingface.co/datasets/sshao0516/CrowdHuman](https://huggingface.co/datasets/sshao0516/CrowdHuman)

2. **Custom Pexels Dataset**

   * 50â€“100 manually collected images  from Pexels and the web
   * All images annotated in **YOLO format** using **MakeSense.ai**

Both datasets are combined during fine-tuning for improved generalization.

---

##  Installation

### Prerequisites

Make sure you have:

* Python â‰¥ 3.8
* PyTorch â‰¥ 2.0
* OpenCV â‰¥ 4.5
* NumPy, Pandas, Matplotlib

### Installation Steps

```bash
# Clone this repository
git clone https://github.com/smze/Virtual_Fence1_Final
cd virtual-fence

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ§¾ Dataset Preparation

### 1ï¸âƒ£ Labeling

Use **MakeSense.ai** or **LabelImg** to annotate people in your custom images or frames.
Save annotations in **YOLO format** (`.txt` files).

### 2ï¸âƒ£ Combining Datasets

Use the provided `combine_datasets.py` script to merge your custom dataset with CrowdHuman:

```bash
python combine_datasets.py
```

### 3ï¸âƒ£ Directory Structure

```
/datasets
  â”œâ”€â”€ crowdhuman/
  â”œâ”€â”€ pexels_custom/
  â””â”€â”€ combined/
```

---

##  Model Training

### YOLOv5 Training

```bash
python train_yolo.py --data data/combined.yaml --epochs 50
```

### MobileNet Training (Custom)

```bash
python train_mobilenet.py --data data/combined --epochs 50
```

### VLM (OMNI) Inference

```bash
python vlm_infer.py --input video.mp4 --zone coordinates.json
```

---

## ğŸ¥ Video Inference & Output

For all three methods, run:

```bash
python main_yolo.py        # YOLOv5 inference
python main_vlm.py         # OMNI-VLM inference
python main_mobilenet.py   # MobileNet inference
```

Each script will:

* Draw bounding boxes around detected people
* Highlight the counting region
* Display the live counter
* Export output as `output_video.mp4`

---

## ğŸ“Š Benchmark & Evaluation

Run the benchmarking script to compare all methods:

```bash
python benchmark.py
```

### Metrics Evaluated

* **Detection Accuracy (mAP)** â€“ Average precision for person detection
* **Counting Accuracy (%)** â€“ Correct count ratio compared to manual annotations
* **Inference Speed (FPS)** â€“ Average frames per second during processing

### Example Output Table

| Model     | mAP (%) | Counting Accuracy (%) | FPS (Raspberry Pi) | FPS (Desktop) |
| --------- | ------- | --------------------- | ------------------ | ------------- |
| YOLOv5    | 92.4    | 95.1                  | 10                 | 35            |
| OMNI VLM  | 90.8    | 94.3                  | 8                  | 28            |
| MobileNet | 88.6    | 91.0                  | **18**             | **45**        |

---

##  Notes on Raspberry Pi Optimization

* MobileNet model is quantized and pruned for faster inference.
* OpenCVâ€™s `cv2.dnn` backend and `cv2.VideoWriter` are used for efficiency.
* Model weights are exported as `.tflite` for TensorFlow Lite compatibility.

---

##  Repository Structure

```
/virtual-fence
â”‚
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ crowdhuman/
â”‚   â”œâ”€â”€ pexels_custom/
â”‚   â””â”€â”€ combined/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ yolov5/
â”‚   â”œâ”€â”€ mobilenet/
â”‚   â””â”€â”€ vlm/
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_yolo.py
â”‚   â”œâ”€â”€ train_mobilenet.py
â”‚   â”œâ”€â”€ vlm_infer.py
â”‚   â”œâ”€â”€ main_yolo.py
â”‚   â”œâ”€â”€ main_mobilenet.py
â”‚   â”œâ”€â”€ main_vlm.py
â”‚   â””â”€â”€ benchmark.py
â”‚
â”œâ”€â”€ output/
â”‚   â””â”€â”€ output_video.mp4
â”‚
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Results Summary

* **YOLOv5** achieved the highest detection precision.
* **OMNI VLM** performed best in crowded or occluded scenes.
* **MobileNet** provided the fastest inference and lowest energy consumption, ideal for Raspberry Pi.

---

## ğŸ’¬ Conclusion

The **Virtual Fence** system successfully detects, tracks, and counts people entering a defined zone using three different approaches.
Among them, **MobileNet** offers the most practical trade-off between speed and accuracy for real-time edge deployment.

This repository includes complete source code, datasets, output samples, and benchmarking scripts for reproducibility.























